{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Importing libraries and loading datasets","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Modelling\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\n\n# KNeighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Perceptron\nfrom sklearn.linear_model import Perceptron\n\n# Support Vector Machines\nfrom sklearn.svm import SVC\n\n# AdaBoost\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:00:30.207158Z","iopub.execute_input":"2021-12-06T21:00:30.207695Z","iopub.status.idle":"2021-12-06T21:00:31.355796Z","shell.execute_reply.started":"2021-12-06T21:00:30.207555Z","shell.execute_reply":"2021-12-06T21:00:31.355124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/titanic/train.csv')\ntest_data = pd.read_csv('../input/titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:00:31.357486Z","iopub.execute_input":"2021-12-06T21:00:31.357964Z","iopub.status.idle":"2021-12-06T21:00:31.395263Z","shell.execute_reply.started":"2021-12-06T21:00:31.357922Z","shell.execute_reply":"2021-12-06T21:00:31.394563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Explore data","metadata":{}},{"cell_type":"code","source":"train_data","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:00:31.396335Z","iopub.execute_input":"2021-12-06T21:00:31.396834Z","iopub.status.idle":"2021-12-06T21:00:31.540439Z","shell.execute_reply.started":"2021-12-06T21:00:31.396791Z","shell.execute_reply":"2021-12-06T21:00:31.539547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:00:31.542538Z","iopub.execute_input":"2021-12-06T21:00:31.542804Z","iopub.status.idle":"2021-12-06T21:00:31.58284Z","shell.execute_reply.started":"2021-12-06T21:00:31.542774Z","shell.execute_reply":"2021-12-06T21:00:31.581913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Columns: \\n{0} \".format(train_data.columns.tolist()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:00:31.584108Z","iopub.execute_input":"2021-12-06T21:00:31.584329Z","iopub.status.idle":"2021-12-06T21:00:31.589697Z","shell.execute_reply.started":"2021-12-06T21:00:31.584302Z","shell.execute_reply":"2021-12-06T21:00:31.588881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Basic data check","metadata":{}},{"cell_type":"markdown","source":"## Missing values","metadata":{}},{"cell_type":"code","source":"missing_values = train_data.isna().any()\nprint('Columns which have missing values: \\n{0}'.format(missing_values[missing_values == True].index.tolist()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:00:31.591483Z","iopub.execute_input":"2021-12-06T21:00:31.592135Z","iopub.status.idle":"2021-12-06T21:00:31.606076Z","shell.execute_reply.started":"2021-12-06T21:00:31.592089Z","shell.execute_reply":"2021-12-06T21:00:31.605221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Percentage of missing values in `Age` column: {0:.2f}\".format(100.*(train_data.Age.isna().sum()/len(train_data))))\nprint(\"Percentage of missing values in `Cabin` column: {0:.2f}\".format(100.*(train_data.Cabin.isna().sum()/len(train_data))))\nprint(\"Percentage of missing values in `Embarked` column: {0:.2f}\".format(100.*(train_data.Embarked.isna().sum()/len(train_data))))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:00:31.607981Z","iopub.execute_input":"2021-12-06T21:00:31.608306Z","iopub.status.idle":"2021-12-06T21:00:31.61884Z","shell.execute_reply.started":"2021-12-06T21:00:31.608256Z","shell.execute_reply":"2021-12-06T21:00:31.617815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check for duplicates","metadata":{}},{"cell_type":"code","source":"duplicates = train_data.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:00:31.620622Z","iopub.execute_input":"2021-12-06T21:00:31.621389Z","iopub.status.idle":"2021-12-06T21:00:31.637814Z","shell.execute_reply.started":"2021-12-06T21:00:31.621336Z","shell.execute_reply":"2021-12-06T21:00:31.636856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical variables","metadata":{}},{"cell_type":"code","source":"categorical = train_data.nunique().sort_values(ascending=True)\nprint('Categorical variables in train data: \\n{0}'.format(categorical))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:00:31.639359Z","iopub.execute_input":"2021-12-06T21:00:31.639882Z","iopub.status.idle":"2021-12-06T21:00:31.65133Z","shell.execute_reply.started":"2021-12-06T21:00:31.639838Z","shell.execute_reply":"2021-12-06T21:00:31.65072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data cleaning","metadata":{}},{"cell_type":"code","source":"def clean_data(data):\n    # Too many missing values\n    data.drop(['Cabin'], axis=1, inplace=True)\n    \n    # Probably will not provide some useful information\n    data.drop(['Name', 'Ticket', 'Fare', 'Embarked'], axis=1, inplace=True)\n    \n    return data\n    \ntrain_data = clean_data(train_data)\ntest_data = clean_data(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:00:31.654293Z","iopub.execute_input":"2021-12-06T21:00:31.654832Z","iopub.status.idle":"2021-12-06T21:00:31.665382Z","shell.execute_reply.started":"2021-12-06T21:00:31.654799Z","shell.execute_reply":"2021-12-06T21:00:31.664624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.tail()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:00:31.666551Z","iopub.execute_input":"2021-12-06T21:00:31.667296Z","iopub.status.idle":"2021-12-06T21:00:31.686395Z","shell.execute_reply.started":"2021-12-06T21:00:31.6672Z","shell.execute_reply":"2021-12-06T21:00:31.685677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Feature engineering\n\nAlthough I have eliminated most of the columns for simplicity, in the future I am planning to recover those columns. They may contain some useful information.  \nFor now encoding the `Sex` column and filling `Age` column is enough to run a model.","metadata":{}},{"cell_type":"code","source":"train_data['Sex'].replace({'male':0, 'female':1}, inplace=True)\ntest_data['Sex'].replace({'male':0, 'female':1}, inplace=True)\n\n# Merge two data to get the average Age and fill the column\nall_data = pd.concat([train_data, test_data])\naverage = all_data.Age.median()\nprint(\"Average Age: {0}\".format(average))\ntrain_data.fillna(value={'Age': average}, inplace=True)\ntest_data.fillna(value={'Age': average}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:00:31.687396Z","iopub.execute_input":"2021-12-06T21:00:31.688083Z","iopub.status.idle":"2021-12-06T21:00:31.702067Z","shell.execute_reply.started":"2021-12-06T21:00:31.688046Z","shell.execute_reply":"2021-12-06T21:00:31.70109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.tail()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:00:31.703474Z","iopub.execute_input":"2021-12-06T21:00:31.703771Z","iopub.status.idle":"2021-12-06T21:00:31.719463Z","shell.execute_reply.started":"2021-12-06T21:00:31.703725Z","shell.execute_reply":"2021-12-06T21:00:31.718499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Modelling\n\nTry different models with different parameters to understand which models give better results.","metadata":{}},{"cell_type":"code","source":"# Set X and y\nX = train_data.drop(['Survived', 'PassengerId'], axis=1)\ny = train_data['Survived']\ntest_X = test_data.drop(['PassengerId'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:00:31.720815Z","iopub.execute_input":"2021-12-06T21:00:31.721071Z","iopub.status.idle":"2021-12-06T21:00:31.733917Z","shell.execute_reply.started":"2021-12-06T21:00:31.721042Z","shell.execute_reply":"2021-12-06T21:00:31.733004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To store models created\nbest_models = {}\n\n# Split data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\ndef print_best_parameters(hyperparameters, best_parameters):\n    value = \"Best parameters: \"\n    for key in hyperparameters:\n        value += str(key) + \": \" + str(best_parameters[key]) + \", \"\n    if hyperparameters:\n        print(value[:-2])\n\ndef get_best_model(estimator, hyperparameters):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    grid_search = GridSearchCV(estimator=estimator, param_grid=hyperparameters, n_jobs=-1, cv=cv, scoring=\"accuracy\")\n    best_model = grid_search.fit(train_X, train_y)\n    best_parameters = best_model.best_estimator_.get_params()\n    print_best_parameters(hyperparameters, best_parameters)\n    return best_model\n\ndef evaluate_model(model, name):\n    print(\"Accuracy score:\", accuracy_score(train_y, model.predict(train_X)))\n    best_models[name] = model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:00:31.770169Z","iopub.execute_input":"2021-12-06T21:00:31.770743Z","iopub.status.idle":"2021-12-06T21:00:31.784126Z","shell.execute_reply.started":"2021-12-06T21:00:31.770697Z","shell.execute_reply":"2021-12-06T21:00:31.783386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Features: \\n{0} \".format(X.columns.tolist()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:00:31.785302Z","iopub.execute_input":"2021-12-06T21:00:31.785983Z","iopub.status.idle":"2021-12-06T21:00:31.795414Z","shell.execute_reply.started":"2021-12-06T21:00:31.785946Z","shell.execute_reply":"2021-12-06T21:00:31.794425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n\nTune the logistic regression model by changing some of its parameters.\n\nLogistic regression parameters:  \n\n* **solver: {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’**  \n    * Algorithm to use in the optimization problem. Default is ‘lbfgs’. To choose a solver, you might want to consider the following aspects:\n        * For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones;\n        * For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss;\n        * ‘liblinear’ is limited to one-versus-rest schemes.\n\n> **Warning**  \n> The choice of the algorithm depends on the penalty chosen: Supported penalties by solver:  \n> * ‘newton-cg’ - [‘l2’, ‘none’]  \n> * ‘lbfgs’ - [‘l2’, ‘none’]  \n> * ‘liblinear’ - [‘l1’, ‘l2’]  \n> * ‘sag’ - [‘l2’, ‘none’]  \n> * ‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’, ‘none’]  \n\n* **penalty: {‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’**  \n    * Specify the norm of the penalty:\n        * 'none': no penalty is added;\n        * 'l2': add a L2 penalty term and it is the default choice;\n        * 'l1': add a L1 penalty term;\n        * 'elasticnet': both L1 and L2 penalty terms are added.\n\n> **Warning**  \n> Some penalties may not work with some solvers. See the parameter solver below, to know the compatibility between the penalty and solver. \n\n* **C: float, default=1.0**  \n    Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n","metadata":{}},{"cell_type":"code","source":"# https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\nhyperparameters = {\n    'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],\n    'penalty' : ['l2'],\n    'C'       : [100, 10, 1.0, 0.1, 0.01]\n}\nestimator = LogisticRegression(random_state=1)\nbest_model_logistic = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:00:31.796658Z","iopub.execute_input":"2021-12-06T21:00:31.797442Z","iopub.status.idle":"2021-12-06T21:00:37.707622Z","shell.execute_reply.started":"2021-12-06T21:00:31.797406Z","shell.execute_reply":"2021-12-06T21:00:37.706449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_logistic.best_estimator_, 'logistic')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:00:37.709272Z","iopub.execute_input":"2021-12-06T21:00:37.709507Z","iopub.status.idle":"2021-12-06T21:00:37.717331Z","shell.execute_reply.started":"2021-12-06T21:00:37.709476Z","shell.execute_reply":"2021-12-06T21:00:37.716748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)","metadata":{}},{"cell_type":"markdown","source":"### [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n\n* **var_smoothing: float, default=1e-9**  \n    Portion of the largest variance of all features that is added to variances for calculation stability.","metadata":{}},{"cell_type":"code","source":"# https://www.analyticsvidhya.com/blog/2021/01/gaussian-naive-bayes-with-hyperpameter-tuning/\nhyperparameters = {\n    'var_smoothing': np.logspace(0, -9, num=100)\n}\nestimator = GaussianNB()\nbest_model_gaussian_nb = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:00:37.718438Z","iopub.execute_input":"2021-12-06T21:00:37.718821Z","iopub.status.idle":"2021-12-06T21:00:43.736912Z","shell.execute_reply.started":"2021-12-06T21:00:37.718776Z","shell.execute_reply":"2021-12-06T21:00:43.736272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_gaussian_nb.best_estimator_, 'gaussian_nb')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:00:43.738929Z","iopub.execute_input":"2021-12-06T21:00:43.739226Z","iopub.status.idle":"2021-12-06T21:00:43.747294Z","shell.execute_reply.started":"2021-12-06T21:00:43.739184Z","shell.execute_reply":"2021-12-06T21:00:43.746348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n\n* **alpha: float, default=1.0**  \n    Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n    \n* **fit_prior: bool, default=True**  \n    Whether to learn class prior probabilities or not. If false, a uniform prior will be used.","metadata":{}},{"cell_type":"code","source":"# https://medium.com/@kocur4d/hyper-parameter-tuning-with-pipelines-5310aff069d6\nhyperparameters = {\n    'alpha'     : [0, 0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n}\nestimator = MultinomialNB()\nbest_model_multinominal_nb = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:01:28.237821Z","iopub.execute_input":"2021-12-06T21:01:28.238126Z","iopub.status.idle":"2021-12-06T21:01:31.81368Z","shell.execute_reply.started":"2021-12-06T21:01:28.238091Z","shell.execute_reply":"2021-12-06T21:01:31.812968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_multinominal_nb.best_estimator_, 'multinominal_nb')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:01:31.815485Z","iopub.execute_input":"2021-12-06T21:01:31.815948Z","iopub.status.idle":"2021-12-06T21:01:31.82315Z","shell.execute_reply.started":"2021-12-06T21:01:31.815916Z","shell.execute_reply":"2021-12-06T21:01:31.822516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [Complement Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html)\n\n* **alpha: float, default=1.0**  \n    Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n\n* **fit_prior: bool, default=True**  \n    Only used in edge case with a single class in the training set.\n\n* **norm: bool, default=False**  \n    Whether or not a second normalization of the weights is performed. The default behavior mirrors the implementations found in Mahout and Weka, which do not follow the full algorithm described in Table 9 of the paper.","metadata":{}},{"cell_type":"code","source":"hyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n    'norm'      : [True, False]\n}\nestimator = ComplementNB()\nbest_model_complement_nb = get_best_model(estimator, hyperparameters)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:01:35.711033Z","iopub.execute_input":"2021-12-06T21:01:35.711526Z","iopub.status.idle":"2021-12-06T21:01:37.112825Z","shell.execute_reply.started":"2021-12-06T21:01:35.711491Z","shell.execute_reply":"2021-12-06T21:01:37.111875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_complement_nb.best_estimator_, 'complement_nb')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:01:42.029095Z","iopub.execute_input":"2021-12-06T21:01:42.029384Z","iopub.status.idle":"2021-12-06T21:01:42.037207Z","shell.execute_reply.started":"2021-12-06T21:01:42.029352Z","shell.execute_reply":"2021-12-06T21:01:42.036213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [Bernoulli Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)\n\n* **alpha: float, default=1.0**  \n    Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n    \n* **fit_prior: bool, default=True**  \n    Whether to learn class prior probabilities or not. If false, a uniform prior will be used.","metadata":{}},{"cell_type":"code","source":"hyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n}\nestimator = BernoulliNB()\nbest_model_bernoulli_nb = get_best_model(estimator, hyperparameters)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:01:49.311169Z","iopub.execute_input":"2021-12-06T21:01:49.313484Z","iopub.status.idle":"2021-12-06T21:01:50.051108Z","shell.execute_reply.started":"2021-12-06T21:01:49.313438Z","shell.execute_reply":"2021-12-06T21:01:50.050251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_bernoulli_nb.best_estimator_, 'bernoulli_nb')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:01:50.710018Z","iopub.execute_input":"2021-12-06T21:01:50.711047Z","iopub.status.idle":"2021-12-06T21:01:50.7191Z","shell.execute_reply.started":"2021-12-06T21:01:50.710999Z","shell.execute_reply":"2021-12-06T21:01:50.718231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [K-nearest neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n\nTune k-nearest neighbors model by changing some of its parameters.\n\n* **n_neighbors: int, default=5**  \n    Number of neighbors to use by default for kneighbors queries.\n\n\n* **weights: {‘uniform’, ‘distance’} or callable, default=’uniform’**  \n    * Weight function used in prediction. Possible values:\n        * ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n        * ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n        * [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n\n\n* **algorithm: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’**  \n    * Algorithm used to compute the nearest neighbors:  \n        * ‘ball_tree’ will use BallTree\n        * ‘kd_tree’ will use KDTree\n        * ‘brute’ will use a brute-force search.\n        * ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n        \n> Note: fitting on sparse input will override the setting of this parameter, using brute force.\n\n\n* **leaf_size: int, default=30**  \n    Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n    \n* **p: int, default=2**  \n    Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n* **n_neighbors: int, default=5**  \n    Number of neighbors to use by default for kneighbors queries.","metadata":{}},{"cell_type":"code","source":"# https://medium.datadriveninvestor.com/k-nearest-neighbors-in-python-hyperparameters-tuning-716734bc557f\nhyperparameters = {\n    'n_neighbors' : list(range(1,5)),\n    'weights'     : ['uniform', 'distance'],\n    'algorithm'   : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'leaf_size'   : list(range(1,10)),\n    'p'           : [1,2]\n}\nestimator = KNeighborsClassifier()\nbest_model_kneighbors = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:00:47.183779Z","iopub.execute_input":"2021-12-06T21:00:47.184216Z","iopub.status.idle":"2021-12-06T21:01:18.789823Z","shell.execute_reply.started":"2021-12-06T21:00:47.184182Z","shell.execute_reply":"2021-12-06T21:01:18.788545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_kneighbors.best_estimator_, 'kneighbors')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:01:18.790815Z","iopub.status.idle":"2021-12-06T21:01:18.791168Z","shell.execute_reply.started":"2021-12-06T21:01:18.790985Z","shell.execute_reply":"2021-12-06T21:01:18.791007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)\n\n* **penalty: {‘l2’,’l1’,’elasticnet’}, default=None**  \n    The penalty (aka regularization term) to be used.\n\n* **max_iter: int, default=1000**  \n    The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit method.\n    \n* **eta0: double, default=1**  \n    Constant by which the updates are multiplied.","metadata":{}},{"cell_type":"code","source":"# https://machinelearningmastery.com/perceptron-algorithm-for-classification-in-python/\n# https://machinelearningmastery.com/manually-optimize-hyperparameters/\nhyperparameters = {\n    'penalty'  : ['l1', 'l2', 'elasticnet'],\n    'eta0'     : [0.0001, 0.001, 0.01, 0.1, 1.0],\n    'max_iter' : list(range(50, 200, 50))\n}\nestimator = Perceptron(random_state=1)\nbest_model_perceptron = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:01:18.792184Z","iopub.status.idle":"2021-12-06T21:01:18.792519Z","shell.execute_reply.started":"2021-12-06T21:01:18.792334Z","shell.execute_reply":"2021-12-06T21:01:18.792359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_perceptron.best_estimator_, 'perceptron')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:01:18.794025Z","iopub.status.idle":"2021-12-06T21:01:18.794381Z","shell.execute_reply.started":"2021-12-06T21:01:18.794183Z","shell.execute_reply":"2021-12-06T21:01:18.794206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Support Vector Machines](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n\n* **C: float, default=1.0**  \n    Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n\n* **kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’**  \n    Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples).\n\n\n* **gamma{‘scale’, ‘auto’} or float, default=’scale’**  \n    Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n    * if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,\n    * if ‘auto’, uses 1 / n_features.","metadata":{}},{"cell_type":"code","source":"# https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/\n# https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167\nhyperparameters = {\n    'C'      : [0.1, 1, 10, 100],\n    'gamma'  : [0.0001, 0.001, 0.01, 0.1, 1],\n    'kernel' : ['rbf']\n}\nestimator = SVC(random_state=1)\nbest_model_svc = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:01:18.796221Z","iopub.status.idle":"2021-12-06T21:01:18.796579Z","shell.execute_reply.started":"2021-12-06T21:01:18.796394Z","shell.execute_reply":"2021-12-06T21:01:18.796417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_svc.best_estimator_, 'svc')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:01:18.798314Z","iopub.status.idle":"2021-12-06T21:01:18.798693Z","shell.execute_reply.started":"2021-12-06T21:01:18.798484Z","shell.execute_reply":"2021-12-06T21:01:18.798508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [AdaBoost Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n\n* **n_estimators: int, default=50**  \n    The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n    \n* **learning_rate: float, default=1.0**  \n    Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. There is a trade-off between the learning_rate and n_estimators parameters.","metadata":{}},{"cell_type":"code","source":"# https://medium.com/@chaudhurysrijani/tuning-of-adaboost-with-computational-complexity-8727d01a9d20\nhyperparameters = {\n    'n_estimators'  : [10, 50, 100, 500],\n    'learning_rate' : [0.001, 0.01, 0.1, 1.0]\n}\nestimator = AdaBoostClassifier(random_state=1)\nbest_model_adaboost = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:01:18.799887Z","iopub.status.idle":"2021-12-06T21:01:18.800228Z","shell.execute_reply.started":"2021-12-06T21:01:18.800041Z","shell.execute_reply":"2021-12-06T21:01:18.800066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_adaboost.best_estimator_, 'adaboost')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:01:18.801437Z","iopub.status.idle":"2021-12-06T21:01:18.801798Z","shell.execute_reply.started":"2021-12-06T21:01:18.801593Z","shell.execute_reply":"2021-12-06T21:01:18.801616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n\nTune decision tree classifier model by changing some of its parameters.\n\n* **criterion: {“gini”, “entropy”}, default=”gini”**  \n    The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n\n* **splitter: {“best”, “random”}, default=”best”**  \n    The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n\n* **max_depth: int, default=None**  \n    The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n    \n\n* **min_samples_split: int or float, default=2**  \n    * The minimum number of samples required to split an internal node:\n        * If int, then consider min_samples_split as the minimum number.\n        * If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n\n\n* **min_samples_leaf: int or float, default=1**  \n   The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.  \n    * If int, then consider min_samples_leaf as the minimum number.\n    * If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.","metadata":{}},{"cell_type":"code","source":"# https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680\n# https://www.kaggle.com/gauravduttakiit/hyperparameter-tuning-in-decision-trees\nhyperparameters = {\n    'criterion'         : ['gini', 'entropy'],\n    'splitter'          : ['best', 'random'],\n    'max_depth'         : [None, 1, 2, 3, 4, 5],\n    'min_samples_split' : list(range(2,5)),\n    'min_samples_leaf'  : list(range(1,5))\n}\nestimator = DecisionTreeClassifier(random_state=1)\nbest_model_decision_tree = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:01:18.802971Z","iopub.status.idle":"2021-12-06T21:01:18.803308Z","shell.execute_reply.started":"2021-12-06T21:01:18.803128Z","shell.execute_reply":"2021-12-06T21:01:18.803153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_decision_tree.best_estimator_, 'decision_tree')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T21:01:18.804498Z","iopub.status.idle":"2021-12-06T21:01:18.804863Z","shell.execute_reply.started":"2021-12-06T21:01:18.804686Z","shell.execute_reply":"2021-12-06T21:01:18.804707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n\n* **n_estimators: int, default=100**  \n    The number of trees in the forest.\n\n\n* **max_features: {“auto”, “sqrt”, “log2”}, int or float, default=”auto”**  \n    * The number of features to consider when looking for the best split:\n        * If int, then consider max_features features at each split.\n        * If float, then max_features is a fraction and round(max_features * n_features) features are considered at each split.\n        * If “auto”, then max_features=sqrt(n_features).\n        * If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n        * If “log2”, then max_features=log2(n_features).\n        * If None, then max_features=n_features.\n\n> Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.\n\n* **criterion: {“gini”, “entropy”}, default=”gini”**  \n    The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific.\n\n* **max_depth: int, default=None**  \n    The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n    \n    \n* **min_samples_split: int or float, default=2**  \n    * The minimum number of samples required to split an internal node:\n        * If int, then consider min_samples_split as the minimum number.\n        * If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n\n\n* **min_samples_leaf: int or float, default=1**  \n    The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.  \n     * If int, then consider min_samples_leaf as the minimum number.\n     * If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.","metadata":{}},{"cell_type":"code","source":"# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n# https://www.analyticsvidhya.com/blog/2020/03/beginners-guide-random-forest-hyperparameter-tuning/\nhyperparameters = {\n    'n_estimators'      : list(range(10, 50, 10)),\n    'max_features'      : ['auto', 'sqrt', 'log2'],\n    'criterion'         : ['gini', 'entropy'],\n    'max_depth'         : [None, 1, 2, 3, 4, 5],\n    'min_samples_split' : list(range(2,5)),\n    'min_samples_leaf'  : list(range(1,5))\n}\nestimator = RandomForestClassifier(random_state=1)\nbest_model_random_forest = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:01:18.806123Z","iopub.status.idle":"2021-12-06T21:01:18.806691Z","shell.execute_reply.started":"2021-12-06T21:01:18.806447Z","shell.execute_reply":"2021-12-06T21:01:18.806474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_random_forest.best_estimator_, 'random_forest')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:01:18.807958Z","iopub.status.idle":"2021-12-06T21:01:18.808348Z","shell.execute_reply.started":"2021-12-06T21:01:18.808156Z","shell.execute_reply":"2021-12-06T21:01:18.808181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WORK IN PROGRESS","metadata":{}},{"cell_type":"markdown","source":"# 7. Submission","metadata":{}},{"cell_type":"code","source":"# Get predictions for each model and create submission files\nfor model in best_models:\n    predictions = best_models[model].predict(test_X)\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n    output.to_csv('submission_' + model + '.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T21:01:18.80959Z","iopub.status.idle":"2021-12-06T21:01:18.810042Z","shell.execute_reply.started":"2021-12-06T21:01:18.80986Z","shell.execute_reply":"2021-12-06T21:01:18.809884Z"},"trusted":true},"execution_count":null,"outputs":[]}]}